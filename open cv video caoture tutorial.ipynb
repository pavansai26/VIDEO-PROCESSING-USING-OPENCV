{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "while(True):\n",
    "  ret,frame=cap.read()\n",
    "  cv2.imshow('frame',frame)\n",
    "  if cv2.waitKey(1)&0xff == ord('q'):\n",
    "    break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "while(True):\n",
    "  ret,frame=cap.read()\n",
    "  gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "  cv2.imshow('frame',gray)\n",
    "  if cv2.waitKey(1)&0xff == ord('q'):\n",
    "    break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame count -1.0\n",
      "frame height 480.0\n",
      "frame width 640.0\n",
      "brightness 50.0\n",
      "auto focus -1.0\n",
      "backlight -1.0\n",
      "contrast 50.0\n",
      "fps 30.0\n"
     ]
    }
   ],
   "source": [
    "cap=cv2.VideoCapture(0)# inplace of zero you can give your video file path\n",
    "while(cap.isOpened()):# when given condition present in brackets is satisfied then code run\n",
    "  ret,frame=cap.read()\n",
    "  gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "  cv2.imshow('frame',gray)\n",
    "  if cv2.waitKey(1)&0xff == ord('q'):\n",
    "    break\n",
    "print('frame count',cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print('frame height',cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print('frame width',cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print('brightness',cap.get(cv2.CAP_PROP_BRIGHTNESS))\n",
    "print('auto focus',cap.get(cv2.CAP_PROP_AUTOFOCUS))\n",
    "print('backlight',cap.get(cv2.CAP_PROP_BACKLIGHT))\n",
    "print('contrast',cap.get(cv2.CAP_PROP_CONTRAST))\n",
    "print('fps',cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame count -1.0\n",
      "frame height 480.0\n",
      "frame width 640.0\n",
      "brightness 50.0\n",
      "auto focus -1.0\n",
      "backlight -1.0\n",
      "contrast 50.0\n",
      "fps 30.0\n"
     ]
    }
   ],
   "source": [
    "cap=cv2.VideoCapture(0)# inplace of zero you can give your video file path\n",
    "fourcc=cv2.VideoWriter_fourcc('X','V','I','D')\n",
    "out=cv2.VideoWriter('output.avi',fourcc,30.0,(640,480))\n",
    "while(cap.isOpened()):# when given condition present in brackets is satisfied then code run\n",
    "  ret,frame=cap.read()\n",
    "  #cv2.line(frame,(0,480),(480,360),(255,0,255),5)\n",
    "  if ret == True:\n",
    "        out.write(frame)\n",
    "  gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "  cv2.imshow('frame',gray)\n",
    "  if cv2.waitKey(1)&0xff == ord('q'):\n",
    "    break\n",
    "print('frame count',cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print('frame height',cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print('frame width',cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print('brightness',cap.get(cv2.CAP_PROP_BRIGHTNESS))\n",
    "print('auto focus',cap.get(cv2.CAP_PROP_AUTOFOCUS))\n",
    "print('backlight',cap.get(cv2.CAP_PROP_BACKLIGHT))\n",
    "print('contrast',cap.get(cv2.CAP_PROP_CONTRAST))\n",
    "print('fps',cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame count -1.0\n",
      "frame height 480.0\n",
      "frame width 640.0\n",
      "brightness 50.0\n",
      "auto focus -1.0\n",
      "backlight -1.0\n",
      "contrast 50.0\n",
      "fps 30.0\n"
     ]
    }
   ],
   "source": [
    "cap=cv2.VideoCapture(0)# inplace of zero you can give your video file path\n",
    "#fourcc=cv2.VideoWriter_fourcc('X','V','I','D')\n",
    "#out=cv2.VideoWriter('output.avi',fourcc,30.0,(640,480))\n",
    "while(cap.isOpened()):# when given condition present in brackets is satisfied then code run\n",
    "  ret,frame=cap.read()\n",
    "  cv2.line(frame,(320,220),(220,500),(255,0,255),5)\n",
    "  cv2.arrowedLine(frame,(100,150),(269,499),(255,0,255),5)\n",
    "  #if ret == True:\n",
    "        #out.write(frame)\n",
    "  gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "  cv2.imshow('frame',gray)\n",
    "  if cv2.waitKey(1)&0xff == ord('q'):\n",
    "    break\n",
    "print('frame count',cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print('frame height',cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print('frame width',cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print('brightness',cap.get(cv2.CAP_PROP_BRIGHTNESS))\n",
    "print('auto focus',cap.get(cv2.CAP_PROP_AUTOFOCUS))\n",
    "print('backlight',cap.get(cv2.CAP_PROP_BACKLIGHT))\n",
    "print('contrast',cap.get(cv2.CAP_PROP_CONTRAST))\n",
    "print('fps',cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "cap.release()\n",
    "#out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame count -1.0\n",
      "frame height 480.0\n",
      "frame width 640.0\n",
      "brightness 50.0\n",
      "auto focus -1.0\n",
      "backlight -1.0\n",
      "contrast 50.0\n",
      "fps 30.0\n"
     ]
    }
   ],
   "source": [
    "cap=cv2.VideoCapture(0)# inplace of zero you can give your video file path\n",
    "#fourcc=cv2.VideoWriter_fourcc('X','V','I','D')\n",
    "#out=cv2.VideoWriter('output.avi',fourcc,30.0,(640,480))\n",
    "while(cap.isOpened()):# when given condition present in brackets is satisfied then code run\n",
    "  ret,frame=cap.read()\n",
    "  #cv2.line(frame,(320,220),(220,500),(255,0,255),5)\n",
    "  #cv2.arrowedLine(frame,(100,150),(269,499),(255,0,255),5)\n",
    "  cv2.rectangle(frame,(384,0),(510,128),(255,0,255),5)\n",
    "  #if ret == True:\n",
    "        #out.write(frame)\n",
    "  gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "  cv2.imshow('frame',gray)\n",
    "  if cv2.waitKey(1)&0xff == ord('q'):\n",
    "    break\n",
    "print('frame count',cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print('frame height',cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print('frame width',cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print('brightness',cap.get(cv2.CAP_PROP_BRIGHTNESS))\n",
    "print('auto focus',cap.get(cv2.CAP_PROP_AUTOFOCUS))\n",
    "print('backlight',cap.get(cv2.CAP_PROP_BACKLIGHT))\n",
    "print('contrast',cap.get(cv2.CAP_PROP_CONTRAST))\n",
    "print('fps',cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "cap.release()\n",
    "#out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame count -1.0\n",
      "frame height 480.0\n",
      "frame width 640.0\n",
      "brightness 50.0\n",
      "auto focus -1.0\n",
      "backlight -1.0\n",
      "contrast 50.0\n",
      "fps 30.0\n"
     ]
    }
   ],
   "source": [
    "cap=cv2.VideoCapture(0)# inplace of zero you can give your video file path\n",
    "#fourcc=cv2.VideoWriter_fourcc('X','V','I','D')\n",
    "#out=cv2.VideoWriter('output.avi',fourcc,30.0,(640,480))\n",
    "while(cap.isOpened()):# when given condition present in brackets is satisfied then code run\n",
    "  ret,frame=cap.read()\n",
    "  #cv2.line(frame,(320,220),(220,500),(255,0,255),5)\n",
    "  #cv2.arrowedLine(frame,(100,150),(269,499),(255,0,255),5)\n",
    "  cv2.rectangle(frame,(384,0),(510,128),(255,0,255),-1)# if you put -1 in the palce of line thickness\n",
    "    # it fits whole object with that color\n",
    "  #if ret == True:\n",
    "        #out.write(frame)\n",
    "  gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "  cv2.imshow('frame',gray)\n",
    "  if cv2.waitKey(1)&0xff == ord('q'):\n",
    "    break\n",
    "print('frame count',cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print('frame height',cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print('frame width',cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print('brightness',cap.get(cv2.CAP_PROP_BRIGHTNESS))\n",
    "print('auto focus',cap.get(cv2.CAP_PROP_AUTOFOCUS))\n",
    "print('backlight',cap.get(cv2.CAP_PROP_BACKLIGHT))\n",
    "print('contrast',cap.get(cv2.CAP_PROP_CONTRAST))\n",
    "print('fps',cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "cap.release()\n",
    "#out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame count -1.0\n",
      "frame height 480.0\n",
      "frame width 640.0\n",
      "brightness 50.0\n",
      "auto focus -1.0\n",
      "backlight -1.0\n",
      "contrast 50.0\n",
      "fps 30.0\n"
     ]
    }
   ],
   "source": [
    "cap=cv2.VideoCapture(0)# inplace of zero you can give your video file path\n",
    "#fourcc=cv2.VideoWriter_fourcc('X','V','I','D')\n",
    "#out=cv2.VideoWriter('output.avi',fourcc,30.0,(640,480))\n",
    "while(cap.isOpened()):# when given condition present in brackets is satisfied then code run\n",
    "  ret,frame=cap.read()\n",
    "  #cv2.line(frame,(320,220),(220,500),(255,0,255),5)\n",
    "  #cv2.arrowedLine(frame,(100,150),(269,499),(255,0,255),5)\n",
    "  cv2.circle(frame,(447,63),63,(255,0,255),-1)# if you put -1 in the palce of line thickness\n",
    "    # it fits whole object with that color\n",
    "  #if ret == True:\n",
    "        #out.write(frame)\n",
    "  gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "  cv2.imshow('frame',gray)\n",
    "  if cv2.waitKey(1)&0xff == ord('q'):\n",
    "    break\n",
    "print('frame count',cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print('frame height',cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print('frame width',cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print('brightness',cap.get(cv2.CAP_PROP_BRIGHTNESS))\n",
    "print('auto focus',cap.get(cv2.CAP_PROP_AUTOFOCUS))\n",
    "print('backlight',cap.get(cv2.CAP_PROP_BACKLIGHT))\n",
    "print('contrast',cap.get(cv2.CAP_PROP_CONTRAST))\n",
    "print('fps',cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "cap.release()\n",
    "#out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)# inplace of zero you can give your video file path\n",
    "#fourcc=cv2.VideoWriter_fourcc('X','V','I','D')\n",
    "#out=cv2.VideoWriter('output.avi',fourcc,30.0,(640,480))\n",
    "while(cap.isOpened()):# when given condition present in brackets is satisfied then code run\n",
    "  ret,frame=cap.read()\n",
    "  #cv2.line(frame,(320,220),(220,500),(255,0,255),5)\n",
    "  #cv2.arrowedLine(frame,(100,150),(269,499),(255,0,255),5)\n",
    "  font=cv2.FONT_HERSHEY_DUPLEX\n",
    "  cv2.putText(frame,'opencv',(10,500),font,4,(255,0,0),10,cv2.LINE_AA)\n",
    "  #if ret == True:\n",
    "        #out.write(frame)\n",
    "  gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "  cv2.imshow('frame',gray)\n",
    "  if cv2.waitKey(1)&0xff == ord('q'):\n",
    "    break\n",
    "#print('frame count',cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#print('frame height',cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "#print('frame width',cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "#print('brightness',cap.get(cv2.CAP_PROP_BRIGHTNESS))\n",
    "#print('auto focus',cap.get(cv2.CAP_PROP_AUTOFOCUS))\n",
    "#print('backlight',cap.get(cv2.CAP_PROP_BACKLIGHT))\n",
    "#print('contrast',cap.get(cv2.CAP_PROP_CONTRAST))\n",
    "#print('fps',cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "cap.release()\n",
    "#out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame count -1.0\n",
      "frame height 480.0\n",
      "frame width 640.0\n",
      "brightness 100.0\n",
      "auto focus -1.0\n",
      "backlight -1.0\n",
      "contrast 100.0\n",
      "fps 30.0\n"
     ]
    }
   ],
   "source": [
    "cap=cv2.VideoCapture(0)# inplace of zero you can give your video file path\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT,500)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,500)\n",
    "cap.set(cv2.CAP_PROP_AUTOFOCUS,-1.0)\n",
    "cap.set(cv2.CAP_PROP_BRIGHTNESS,100.0)\n",
    "cap.set(cv2.CAP_PROP_CONTRAST,100.0)\n",
    "cap.set(cv2.CAP_PROP_FPS,20.0)\n",
    "#fourcc=cv2.VideoWriter_fourcc('X','V','I','D')\n",
    "#out=cv2.VideoWriter('output.avi',fourcc,30.0,(640,480))\n",
    "while(cap.isOpened()):# when given condition present in brackets is satisfied then code run\n",
    "  ret,frame=cap.read()\n",
    "  #cv2.line(frame,(320,220),(220,500),(255,0,255),5)\n",
    "  #cv2.arrowedLine(frame,(100,150),(269,499),(255,0,255),5)\n",
    "  #cv2.circle(frame,(447,63),63,(255,0,255),-1)# if you put -1 in the palce of line thickness\n",
    "    # it fits whole object with that color\n",
    "  #if ret == True:\n",
    "        #out.write(frame)\n",
    "  gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "  cv2.imshow('frame',gray)\n",
    "  if cv2.waitKey(1)&0xff == ord('q'):\n",
    "    break\n",
    "print('frame count',cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print('frame height',cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print('frame width',cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print('brightness',cap.get(cv2.CAP_PROP_BRIGHTNESS))\n",
    "print('auto focus',cap.get(cv2.CAP_PROP_AUTOFOCUS))\n",
    "print('backlight',cap.get(cv2.CAP_PROP_BACKLIGHT))\n",
    "print('contrast',cap.get(cv2.CAP_PROP_CONTRAST))\n",
    "print('fps',cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "cap.release()\n",
    "#out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame count -1.0\n",
      "frame height 480.0\n",
      "frame width 640.0\n",
      "brightness 100.0\n",
      "auto focus -1.0\n",
      "backlight -1.0\n",
      "contrast 100.0\n",
      "fps 30.0\n"
     ]
    }
   ],
   "source": [
    "cap=cv2.VideoCapture(0)# inplace of zero you can give your video file path\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT,500)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,500)\n",
    "cap.set(cv2.CAP_PROP_AUTOFOCUS,-1.0)\n",
    "cap.set(cv2.CAP_PROP_BRIGHTNESS,100.0)\n",
    "cap.set(cv2.CAP_PROP_CONTRAST,100.0)\n",
    "cap.set(cv2.CAP_PROP_FPS,20.0)\n",
    "#fourcc=cv2.VideoWriter_fourcc('X','V','I','D')\n",
    "#out=cv2.VideoWriter('output.avi',fourcc,30.0,(640,480))\n",
    "while(cap.isOpened()):# when given condition present in brackets is satisfied then code run\n",
    "  ret,frame=cap.read()\n",
    "  font=cv2.FONT_HERSHEY_DUPLEX\n",
    "  text='width'+str(cap.get(3))+'height'+str(cap.get(4))\n",
    "  cv2.putText(frame,text,(10,500),font,4,(255,0,0),10,cv2.LINE_8)\n",
    "  #cv2.line(frame,(320,220),(220,500),(255,0,255),5)\n",
    "  #cv2.arrowedLine(frame,(100,150),(269,499),(255,0,255),5)\n",
    "  #cv2.circle(frame,(447,63),63,(255,0,255),-1)# if you put -1 in the palce of line thickness\n",
    "    # it fits whole object with that color\n",
    "  #if ret == True:\n",
    "        #out.write(frame)\n",
    "  gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "  cv2.imshow('frame',gray)\n",
    "  if cv2.waitKey(1)&0xff == ord('q'):\n",
    "    break\n",
    "print('frame count',cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print('frame height',cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print('frame width',cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print('brightness',cap.get(cv2.CAP_PROP_BRIGHTNESS))\n",
    "print('auto focus',cap.get(cv2.CAP_PROP_AUTOFOCUS))\n",
    "print('backlight',cap.get(cv2.CAP_PROP_BACKLIGHT))\n",
    "print('contrast',cap.get(cv2.CAP_PROP_CONTRAST))\n",
    "print('fps',cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "cap.release()\n",
    "#out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e641c612fc94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#out=cv2.VideoWriter('output.avi',fourcc,30.0,(640,480))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m# when given condition present in brackets is satisfied then code run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m   \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m   \u001b[0mfont\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFONT_HERSHEY_DUPLEX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m   \u001b[1;31m#text='width'+str(cap.get(3))+'height'+str(cap.get(4))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "cap=cv2.VideoCapture(0)# inplace of zero you can give your video file path\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT,500)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,500)\n",
    "cap.set(cv2.CAP_PROP_AUTOFOCUS,-1.0)\n",
    "cap.set(cv2.CAP_PROP_BRIGHTNESS,100.0)\n",
    "cap.set(cv2.CAP_PROP_CONTRAST,100.0)\n",
    "cap.set(cv2.CAP_PROP_FPS,20.0)\n",
    "#fourcc=cv2.VideoWriter_fourcc('X','V','I','D')\n",
    "#out=cv2.VideoWriter('output.avi',fourcc,30.0,(640,480))\n",
    "while(cap.isOpened()):# when given condition present in brackets is satisfied then code run\n",
    "  ret,frame=cap.read()\n",
    "  font=cv2.FONT_HERSHEY_DUPLEX\n",
    "  #text='width'+str(cap.get(3))+'height'+str(cap.get(4))\n",
    "  datet=str(datetime.datetime.now())\n",
    "  cv2.putText(frame,datet,(10,500),font,4,(255,0,0),10,cv2.LINE_8)#put datetime in video\n",
    "  #cv2.line(frame,(320,220),(220,500),(255,0,255),5)\n",
    "  #cv2.arrowedLine(frame,(100,150),(269,499),(255,0,255),5)\n",
    "  #cv2.circle(frame,(447,63),63,(255,0,255),-1)# if you put -1 in the palce of line thickness\n",
    "    # it fits whole object with that color\n",
    "  #if ret == True:\n",
    "        #out.write(frame)\n",
    "  gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "  cv2.imshow('frame',gray)\n",
    "  if cv2.waitKey(1)&0xff == ord('q'):\n",
    "    break\n",
    "print('frame count',cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print('frame height',cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print('frame width',cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print('brightness',cap.get(cv2.CAP_PROP_BRIGHTNESS))\n",
    "print('auto focus',cap.get(cv2.CAP_PROP_AUTOFOCUS))\n",
    "print('backlight',cap.get(cv2.CAP_PROP_BACKLIGHT))\n",
    "print('contrast',cap.get(cv2.CAP_PROP_CONTRAST))\n",
    "print('fps',cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "cap.release()\n",
    "#out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EVENT_FLAG_ALTKEY', 'EVENT_FLAG_CTRLKEY', 'EVENT_FLAG_LBUTTON', 'EVENT_FLAG_MBUTTON', 'EVENT_FLAG_RBUTTON', 'EVENT_FLAG_SHIFTKEY', 'EVENT_LBUTTONDBLCLK', 'EVENT_LBUTTONDOWN', 'EVENT_LBUTTONUP', 'EVENT_MBUTTONDBLCLK', 'EVENT_MBUTTONDOWN', 'EVENT_MBUTTONUP', 'EVENT_MOUSEHWHEEL', 'EVENT_MOUSEMOVE', 'EVENT_MOUSEWHEEL', 'EVENT_RBUTTONDBLCLK', 'EVENT_RBUTTONDOWN', 'EVENT_RBUTTONUP']\n"
     ]
    }
   ],
   "source": [
    "events=[i for i in dir(cv2) if 'EVENT' in i]\n",
    "print(events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345 ,  312\n",
      "97 ,  108\n",
      "130 ,  193\n",
      "100 ,  283\n",
      "106 ,  402\n"
     ]
    }
   ],
   "source": [
    "def click_event(event,x,y,flags,param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        print(x,', ',y)\n",
    "        font=cv2.FONT_HERSHEY_COMPLEX\n",
    "        strx=str(x)+', '+str(y)\n",
    "        cv2.putText(img,strx,(x,y),font,1,(255,255,0),2)\n",
    "        cv2.imshow('image',img)\n",
    "img=np.zeros((512,512,3),np.uint8)\n",
    "cv2.imshow('image',img)\n",
    "cv2.setMouseCallback('image',click_event)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mouse click events\n",
    "def click_event(event,x,y,flags,param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        print(x,', ',y)\n",
    "        font=cv2.FONT_HERSHEY_COMPLEX\n",
    "        strx=str(x)+', '+str(y)\n",
    "        cv2.putText(img,strx,(x,y),font,1,(255,255,0),2)\n",
    "        cv2.imshow('image',img)\n",
    "    if event == cv2.EVENT_RBUTTONDOWN:\n",
    "        blue=img[y,x,0]\n",
    "        green=img[y,x,1]\n",
    "        red=img[y,x,2]\n",
    "        font=cv2.FONT_HERSHEY_COMPLEX\n",
    "        strx=str(blue)+', '+str(green)+', '+str(red)\n",
    "        cv2.putText(img,strx,(x,y),font,1,(255,255,0),2)\n",
    "        cv2.imshow('image',img) \n",
    "        \n",
    "img=np.zeros((512,512,3),np.uint8)\n",
    "cv2.imshow('image',img)        \n",
    "cv2.setMouseCallback('image',click_event)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_event(event,x,y,flags,param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "       cv2.circle(img,(x,y),3,(0,0,255),-1)\n",
    "       points.append((x,y))\n",
    "       if len(points)>=2:\n",
    "        cv2.line(img,points[-1],points[-2],(255,0,0),5)\n",
    "    cv2.imshow('image',img)\n",
    "    \n",
    "        \n",
    "img=cv2.imread('image',frame)\n",
    "cv2.imshow('image',img)\n",
    "points=[]\n",
    "cv2.setMouseCallback('image',click_event)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_event(event,x,y,flags,param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        blue=img[x,y,0]\n",
    "        green=img[x,y,1]\n",
    "        red=img[x,y,2]\n",
    "        cv2.circle(img,(x,y),3,(0,0,255),-1)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'tk' from 'tkinter' (C:\\Users\\lenovo\\Anaconda3\\lib\\tkinter\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3b15f7ca01e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#import tkinter as tk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#root=tk.Tk()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtkinter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtkinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfiledialog\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maskopenfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# we don't want a full GUI, so keep the root window from appearing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'tk' from 'tkinter' (C:\\Users\\lenovo\\Anaconda3\\lib\\tkinter\\__init__.py)"
     ]
    }
   ],
   "source": [
    "#import tkinter as tk\n",
    "#root=tk.Tk()\n",
    "from tkinter import tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "tk().withdraw() # we don't want a full GUI, so keep the root window from appearing\n",
    "filename = askopenfilename() # show an \"Open\" dialog box and return the path to the selected file\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tkinter as tk\n",
    "#root=tk.Tk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter.filedialog import askopenfilename\n",
    "filename = askopenfilename()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from h5py) (1.16.2)\n",
      "Requirement already satisfied: six in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from h5py) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imageai==2.0.1 from https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.1/imageai-2.0.1-py3-none-any.whl\n",
      "  Downloading https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.1/imageai-2.0.1-py3-none-any.whl (137kB)\n",
      "Installing collected packages: imageai\n",
      "  Found existing installation: imageai 2.0.2\n",
      "    Uninstalling imageai-2.0.2:\n",
      "      Successfully uninstalled imageai-2.0.2\n",
      "Successfully installed imageai-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.1/imageai-2.0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imageai==2.0.2 from https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl\n",
      "  Downloading https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl (151kB)\n",
      "Installing collected packages: imageai\n",
      "  Found existing installation: imageai 2.0.1\n",
      "    Uninstalling imageai-2.0.1:\n",
      "      Successfully uninstalled imageai-2.0.1\n",
      "Successfully installed imageai-2.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_BRIGHTNESS,50.0)\n",
    "cap.set(cv2.CAP_PROP_CONTRAST,50.0)\n",
    "while(True):\n",
    "  ret,frame=cap.read()\n",
    "  cv2.imshow('frame',frame)\n",
    "  if cv2.waitKey(1)&0xff == ord('q'):\n",
    "    break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from imageai.Detection import ObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = ObjectDetection()\n",
    "detector.setModelTypeAsRetinaNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageai.Detection import VideoObjectDetection\n",
    "import os\n",
    "execution_path = os.getcwd()\n",
    "detector = VideoObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath( os.path.join(execution_path , \"yolo.h5\"))\n",
    "detector.loadModel()\n",
    "video_path = detector.detectObjectsFromVideo(input_file_path=os.path.join(execution_path, \"movie.mp4\"),\n",
    "                            output_file_path=os.path.join(execution_path, \"movie_detected\")\n",
    "                            , frames_per_second=30, log_progress=True)\n",
    "print(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = detector.CustomObjects(person=True,chair=True,cell_phone=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core._api.v2.image' has no attribute 'resize_images'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-709c279229b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetModelTypeAsRetinaNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetModelPath\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecution_path\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m\"resnet50_coco_best_v2.0.1.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mcustom_objects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCustomObjects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchair\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcell_phone\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m detections = detector.detectCustomObjectsFromImage(input_image=frame,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\__init__.py\u001b[0m in \u001b[0;36mloadModel\u001b[1;34m(self, detection_speed)\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You must set a valid model type before loading the model.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__modelType\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"retinanet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresnet50_retinanet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodelPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__model_collection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\keras_retinanet\\models\\resnet.py\u001b[0m in \u001b[0;36mresnet50_retinanet\u001b[1;34m(num_classes, inputs, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mresnet50_retinanet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mresnet_retinanet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'resnet50'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\keras_retinanet\\models\\resnet.py\u001b[0m in \u001b[0;36mresnet_retinanet\u001b[1;34m(num_classes, backbone, inputs, modifier, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;31m# create the full model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mretinanet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretinanet_bbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\keras_retinanet\\models\\retinanet.py\u001b[0m in \u001b[0;36mretinanet_bbox\u001b[1;34m(inputs, num_classes, nms, name, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m     \"\"\"\n\u001b[1;32m--> 347\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mretinanet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[1;31m# we expect the anchors, regression and classification values as first output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\keras_retinanet\\models\\retinanet.py\u001b[0m in \u001b[0;36mretinanet\u001b[1;34m(inputs, backbone, num_classes, anchor_parameters, create_pyramid_features, submodels, name)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[1;31m# compute pyramid features as per https://arxiv.org/abs/1708.02002\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_pyramid_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;31m# for all pyramid levels, run available submodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\keras_retinanet\\models\\retinanet.py\u001b[0m in \u001b[0;36m__create_pyramid_features\u001b[1;34m(C3, C4, C5, feature_size)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;31m# upsample C5 to get P5 from the FPN paper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[0mP5\u001b[0m           \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C5_reduced'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0mP5_upsampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUpsampleLike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'P5_upsampled'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mP5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     \u001b[0mP5\u001b[0m           \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'P5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[1;31m# Actually call the layer,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[1;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\keras_retinanet\\layers\\_misc.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[0mtarget_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtarget_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\keras_retinanet\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mresize_images\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mresize_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core._api.v2.image' has no attribute 'resize_images'"
     ]
    }
   ],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_BRIGHTNESS,50.0)\n",
    "cap.set(cv2.CAP_PROP_CONTRAST,50.0)\n",
    "while(True):\n",
    "  ret,frame=cap.read()\n",
    "  cv2.imshow('frame',frame)\n",
    "  if cv2.waitKey(1)&0xff == ord('q'):\n",
    "    break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "import os\n",
    "execution_path = os.getcwd()\n",
    "detector = ObjectDetection()\n",
    "detector.setModelTypeAsRetinaNet()\n",
    "detector.setModelPath( os.path.join(execution_path , \"resnet50_coco_best_v2.0.1.h5\"))\n",
    "detector.loadModel()\n",
    "custom_objects = detector.CustomObjects(person=True,chair=True,cell_phone=True,)\n",
    "detections = detector.detectCustomObjectsFrom(input_image=frame,\n",
    "                                                   custom_objects=custom_objects,\n",
    "                                                   minimum_percentage_probability=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "`get_session` is not available when using TensorFlow 2.0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-78e3880032bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mexecution_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcamera\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdetector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVideoObjectDetection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetModelTypeAsYOLOv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetModelPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecution_path\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m\"yolo.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    912\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__yolo_model_image_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m416\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m416\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__yolo_boxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__yolo_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__yolo_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m         \u001b[1;31m# Unique instance variables for TinyYOLOv3.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[1;34m()\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_is_tf_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         raise RuntimeError(\n\u001b[1;32m--> 379\u001b[1;33m             \u001b[1;34m'`get_session` is not available '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m             'when using TensorFlow 2.0.')\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: `get_session` is not available when using TensorFlow 2.0."
     ]
    }
   ],
   "source": [
    "from imageai.Detection import VideoObjectDetection\n",
    "import os\n",
    "import cv2\n",
    "execution_path = os.getcwd()\n",
    "camera = cv2.VideoCapture(0)\n",
    "detector = VideoObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath(os.path.join(execution_path , \"yolo.h5\"))\n",
    "detector.loadModel()\n",
    "video_path = detector.detectObjectsFromVideo(camera_input=camera,\n",
    "    output_file_path=os.path.join(execution_path, \"camera_detected_video\")\n",
    "    , frames_per_second=30, log_progress=True, minimum_percentage_probability=70)\n",
    "\n",
    "print(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core._api.v2.image' has no attribute 'resize_images'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-95bc704d10bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetModelTypeAsRetinaNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetModelPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecution_path\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m\"resnet50_coco_best_v2.0.1.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m video_path = detector.detectObjectsFromVideo(camera_input=camera,\n\u001b[0;32m     11\u001b[0m                         \u001b[0moutput_file_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecution_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"camera_detected_video\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\__init__.py\u001b[0m in \u001b[0;36mloadModel\u001b[1;34m(self, detection_speed)\u001b[0m\n\u001b[0;32m    586\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You must set a valid model type before loading the model.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m             \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__modelType\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"retinanet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresnet50_retinanet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodelPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__model_collection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\keras_retinanet\\models\\resnet.py\u001b[0m in \u001b[0;36mresnet50_retinanet\u001b[1;34m(num_classes, inputs, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mresnet50_retinanet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mresnet_retinanet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'resnet50'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\keras_retinanet\\models\\resnet.py\u001b[0m in \u001b[0;36mresnet_retinanet\u001b[1;34m(num_classes, backbone, inputs, modifier, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;31m# create the full model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mretinanet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretinanet_bbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\keras_retinanet\\models\\retinanet.py\u001b[0m in \u001b[0;36mretinanet_bbox\u001b[1;34m(inputs, num_classes, nms, name, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m     \"\"\"\n\u001b[1;32m--> 347\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mretinanet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[1;31m# we expect the anchors, regression and classification values as first output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\keras_retinanet\\models\\retinanet.py\u001b[0m in \u001b[0;36mretinanet\u001b[1;34m(inputs, backbone, num_classes, anchor_parameters, create_pyramid_features, submodels, name)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[1;31m# compute pyramid features as per https://arxiv.org/abs/1708.02002\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_pyramid_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;31m# for all pyramid levels, run available submodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\keras_retinanet\\models\\retinanet.py\u001b[0m in \u001b[0;36m__create_pyramid_features\u001b[1;34m(C3, C4, C5, feature_size)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;31m# upsample C5 to get P5 from the FPN paper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[0mP5\u001b[0m           \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C5_reduced'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0mP5_upsampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUpsampleLike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'P5_upsampled'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mP5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     \u001b[0mP5\u001b[0m           \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'P5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[1;31m# Actually call the layer,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[1;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\keras_retinanet\\layers\\_misc.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[0mtarget_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtarget_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageai\\Detection\\keras_retinanet\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mresize_images\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mresize_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core._api.v2.image' has no attribute 'resize_images'"
     ]
    }
   ],
   "source": [
    "from imageai.Detection import VideoObjectDetection\n",
    "import os\n",
    "import cv2\n",
    "execution_path = os.getcwd()\n",
    "camera = cv2.VideoCapture(0)\n",
    "detector = VideoObjectDetection()\n",
    "detector.setModelTypeAsRetinaNet()\n",
    "detector.setModelPath(os.path.join(execution_path , \"resnet50_coco_best_v2.0.1.h5\"))\n",
    "detector.loadModel()\n",
    "video_path = detector.detectObjectsFromVideo(camera_input=camera,\n",
    "                        output_file_path=os.path.join(execution_path, \"camera_detected_video\")\n",
    "                        , frames_per_second=20, log_progress=True, minimum_percentage_probability=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VideoObjectDetection' object has no attribute 'setModelTypeAsYolov3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-416782c74cd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcamera\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdetector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVideoObjectDetection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetModelTypeAsYolov3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetModelPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecution_path\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m\"yolo.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'VideoObjectDetection' object has no attribute 'setModelTypeAsYolov3'"
     ]
    }
   ],
   "source": [
    "from imageai.Detection import VideoObjectDetection\n",
    "import os\n",
    "import cv2\n",
    "execution_path = os.getcwd()\n",
    "camera = cv2.VideoCapture(0)\n",
    "detector = VideoObjectDetection()\n",
    "detector.setModelTypeAsRetinaNet()\n",
    "detector.setModelPath(os.path.join(execution_path , ))\n",
    "detector.loadModel()\n",
    "video_path = detector.detectObjectsFromVideo(camera_input=camera,\n",
    "    output_file_path=os.path.join(execution_path, \"camera_detected_video\")\n",
    "    , frames_per_second=20, log_progress=True, minimum_percentage_probability=30)\n",
    "\n",
    "print(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "detectObjectsFromVideo() got an unexpected keyword argument 'camera_input'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-58dbe27e3d26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m video_path = detector.detectObjectsFromVideo(camera_input=camera,\n\u001b[0;32m     11\u001b[0m                         \u001b[0moutput_file_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecution_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"camera_detected_video\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                         , frames_per_second=20, log_progress=True, minimum_percentage_probability=70)\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: detectObjectsFromVideo() got an unexpected keyword argument 'camera_input'"
     ]
    }
   ],
   "source": [
    "from imageai.Detection import VideoObjectDetection\n",
    "import os\n",
    "import cv2\n",
    "execution_path = os.getcwd()\n",
    "camera = cv2.VideoCapture(0)\n",
    "detector = VideoObjectDetection()\n",
    "detector.setModelTypeAsRetinaNet()\n",
    "detector.setModelPath(os.path.join(execution_path , \"resnet50_coco_best_v2.0.1.h5\"))\n",
    "#detector.loadModel()\n",
    "video_path = detector.detectObjectsFromVideo(camera_input=camera,\n",
    "                        output_file_path=os.path.join(execution_path, \"camera_detected_video\")\n",
    "                        , frames_per_second=20, log_progress=True, minimum_percentage_probability=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import pandas as pd\n",
    "# importing datetime class from datetime library \n",
    "from datetime import datetime \n",
    "# List when any moving object appear \n",
    "#motion_list = [ None, None ] \n",
    "# Time of movement \n",
    "#time = [] \n",
    "# Initializing DataFrame, one column is start  \n",
    "# time and other column is end time \n",
    "#df = pd.DataFrame(columns = [\"Start\", \"End\"])\n",
    "\n",
    "first_frame = None\n",
    "# Create a VideoCApture object to record video using web Cam\n",
    "video  = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    check, frame = video.read()\n",
    "    # Initializing motion = 0(no motion) \n",
    "   # motion = 0\n",
    "    \n",
    "    # Convert the frame to gray scale\n",
    "    gray= cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Convert the gray scale image to GaussianBlur\n",
    "    gray = cv2.GaussianBlur(gray, (21,21),0)\n",
    "    \n",
    "    # To store first image/frame of videos None:\n",
    "    if first_frame is None:\n",
    "        first_frame = gray\n",
    "        continue\n",
    "        \n",
    "    # Calculate the difference between the first frame and other frames\n",
    "    delta_frame = cv2.absdiff(first_frame, gray)\n",
    "    \n",
    "    # Provides a threshold value, such that it will convert \n",
    "    # the difference value with less than 30 to black.\n",
    "    # If the difference is greater than 30 it will convert \n",
    "    # those pixels to white\n",
    "    thresh_delta = cv2.threshold(delta_frame, 30,255, cv2.THRESH_BINARY)[1]\n",
    "    thresh_delta = cv2.dilate(thresh_delta , None , iterations = 0)\n",
    "    \n",
    "    # Define the contour area.\n",
    "    # Basically, add the borders.\n",
    "    #cnts= cv2.findContours(thresh_delta.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    _, cnts,val = cv2.findContours(gray, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Remove noises and shadows. Basically it will keep only that \n",
    "    # part white, which has area greater than 1000 pixels.\n",
    "    \n",
    "    for contour in cnts:\n",
    "        if cv2.contourArea(contour) <1000:\n",
    "            continue\n",
    "        #motion = 1\n",
    "        \n",
    "        # Creates a rectangular box around the object in the frame\n",
    "        (x,y,w,h) = cv2.boundingRect(contour)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 3)\n",
    "         # Appending status of motion \n",
    "    #motion_list.append(motion) \n",
    "    #motion_list = motion_list[-2:] \n",
    "    # Appending Start time of motion \n",
    "    #if motion_list[-1] == 1 and motion_list[-2] == 0: \n",
    "        #time.append(datetime.now()) \n",
    "  \n",
    "    # Appending End time of motion \n",
    "    #if motion_list[-1] == 0 and motion_list[-2] == 1: \n",
    "        #time.append(datetime.now()) \n",
    "  \n",
    "        \n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.imshow('capturing', gray)\n",
    "    cv2.imshow('delta',delta_frame)\n",
    "    cv2.imshow('thresh', thresh_delta)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        # if something is movingthen it append the end time of movement \n",
    "        #if motion == 1: \n",
    "            #time.append(datetime.now()) \n",
    "        break\n",
    "# Appending time of motion in DataFrame \n",
    "#for i in range(0, len(time), 2): \n",
    "    #df = df.append({\"Start\":time[i], \"End\":time[i + 1]}, ignore_index = True) \n",
    "  \n",
    "# Creating a csv file in which time of movements will be saved \n",
    "#df.to_csv(\"Time_of_movements.csv\") \n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "imshow() missing required argument 'mat' (pos 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-392d64cdf776>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Detection\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgray1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;36m0xFF\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'q'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: imshow() missing required argument 'mat' (pos 2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    frame = cv2.resize(frame, (640, 480), interpolation = cv2.INTER_LINEAR)\n",
    "    gray1=frame # Is this right?\n",
    "\n",
    "    frameArea = frame.shape[0]*frame.shape[1]\n",
    "\n",
    "    # split the RGB image into R,G,B channels respectively\n",
    "    b, g, r = frame[:, :, 0], frame[:, :, 1], frame[:, :, 2]\n",
    "\n",
    "    # put back thresholded channels into one RGB image\n",
    "    retvalb, b = cv2.threshold(b, 90, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    retvalg,g = cv2.threshold(g, 0, 70, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    retvalr,r = cv2.threshold(r, 0, 70, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "    frame = cv2.merge((b,g,r))\n",
    "\n",
    "    # create gray image in order to further threshold the result\n",
    "    gray1 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY);\n",
    "    retvalgray, gray = cv2.threshold(gray1,0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU);\n",
    "\n",
    "    gray1 = cv2.bilateralFilter(gray1, 11, 17, 17)\n",
    "    gray1 = cv2.convertScaleAbs(gray1)\n",
    "    # find the region of interest by drawing contours around it\n",
    "    _, cnts,val = cv2.findContours(gray1, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) #gray / edged ?\n",
    "\n",
    "    for cnt in cnts:\n",
    "                eps=cv2.arcLength(cnt,True)\n",
    "                approx = cv2.approxPolyDP(cnt,0.01*eps,True)\n",
    "                cv2.drawContours(gray1,[cnt],0,(0,0,255, 1),3)\n",
    "\n",
    "    cv2.imshow(\"Detection\", gray1)\n",
    "    #cv2.imshow(frame)\n",
    "    if cv2.waitKey(1) & 0xFF is ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            print(\"Stop programm and close all windows\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'thresh_delta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7ef4c7d96179>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mthresh_delta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'thresh_delta' is not defined"
     ]
    }
   ],
   "source": [
    "thresh_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4ef7b9668872>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;31m# Appending time of motion in DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"Start\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"End\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;31m# Creating a csv file in which time of movements will be saved\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import pandas as pd\n",
    "# importing datetime class from datetime library \n",
    "from datetime import datetime \n",
    "# List when any moving object appear \n",
    "motion_list = [ None, None ] \n",
    "# Time of movement \n",
    "time = [] \n",
    "# Initializing DataFrame, one column is start  \n",
    "# time and other column is end time \n",
    "df = pd.DataFrame(columns = [\"Start\", \"End\"])\n",
    "\n",
    "first_frame = None\n",
    "# Create a VideoCApture object to record video using web Cam\n",
    "video  = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    check, frame = video.read()\n",
    "    # Initializing motion = 0(no motion) \n",
    "    motion = 0\n",
    "    \n",
    "    # Convert the frame to gray scale\n",
    "    gray= cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Convert the gray scale image to GaussianBlur\n",
    "    gray = cv2.GaussianBlur(gray, (21,21),0)\n",
    "    \n",
    "    # To store first image/frame of videos None:\n",
    "    if first_frame is None:\n",
    "        first_frame = gray\n",
    "        continue\n",
    "        \n",
    "    # Calculate the difference between the first frame and other frames\n",
    "    delta_frame = cv2.absdiff(first_frame, gray)\n",
    "    \n",
    "    # Provides a threshold value, such that it will convert \n",
    "    # the difference value with less than 30 to black.\n",
    "    # If the difference is greater than 30 it will convert \n",
    "    # those pixels to white\n",
    "    thresh_delta = cv2.threshold(delta_frame, 30,255, cv2.THRESH_BINARY)[1]\n",
    "    thresh_delta = cv2.dilate(thresh_delta , None , iterations = 0)\n",
    "    \n",
    "    # Define the contour area.\n",
    "    # Basically, add the borders.\n",
    "    #cnts= cv2.findContours(thresh_delta.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    _, cnts,val = cv2.findContours(gray, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Remove noises and shadows. Basically it will keep only that \n",
    "    # part white, which has area greater than 1000 pixels.\n",
    "    \n",
    "    for contour in cnts:\n",
    "        if cv2.contourArea(contour) <1000:\n",
    "            continue\n",
    "        motion = 1\n",
    "        \n",
    "        # Creates a rectangular box around the object in the frame\n",
    "        (x,y,w,h) = cv2.boundingRect(contour)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 3)\n",
    "         # Appending status of motion \n",
    "    motion_list.append(motion) \n",
    "    motion_list = motion_list[-2:] \n",
    "    # Appending Start time of motion \n",
    "    if motion_list[-1] == 1 and motion_list[-2] == 0: \n",
    "        time.append(datetime.now()) \n",
    "  \n",
    "    # Appending End time of motion \n",
    "    if motion_list[-1] == 0 and motion_list[-2] == 1: \n",
    "        time.append(datetime.now()) \n",
    "    \n",
    "        \n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.imshow('capturing', gray)\n",
    "    cv2.imshow('delta',delta_frame)\n",
    "    cv2.imshow('thresh', thresh_delta)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        # if something is movingthen it append the end time of movement \n",
    "        if motion == 1: \n",
    "            time.append(datetime.now()) \n",
    "            break\n",
    "# Appending time of motion in DataFrame \n",
    "for i in range(0, len(time), 2): \n",
    "    df = df.append({\"Start\":time[i], \"End\":time[i + 1]}, ignore_index = True) \n",
    "  \n",
    "# Creating a csv file in which time of movements will be saved \n",
    "print(motion_list)\n",
    "df.to_csv(\"Time_of_movements.csv\") \n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Python program to implement \n",
    "# WebCam Motion Detector \n",
    "\n",
    "# importing OpenCV, time and Pandas library \n",
    "import cv2, time, pandas \n",
    "# importing datetime class from datetime library \n",
    "from datetime import datetime \n",
    "\n",
    "# Assigning our static_back to None \n",
    "static_back = None\n",
    "\n",
    "# List when any moving object appear \n",
    "motion_list = [ None, None ] \n",
    "\n",
    "# Time of movement \n",
    "time = [] \n",
    "\n",
    "# Initializing DataFrame, one column is start \n",
    "# time and other column is end time \n",
    "df = pandas.DataFrame(columns = [\"Start\", \"End\"]) \n",
    "\n",
    "# Capturing video \n",
    "video = cv2.VideoCapture(0) \n",
    "\n",
    "# Infinite while loop to treat stack of image as video \n",
    "while True: \n",
    "\t# Reading frame(image) from video \n",
    "\tcheck, frame = video.read() \n",
    "\n",
    "\t# Initializing motion = 0(no motion) \n",
    "\tmotion = 0\n",
    "\n",
    "\t# Converting color image to gray_scale image \n",
    "\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) \n",
    "\n",
    "\t# Converting gray scale image to GaussianBlur \n",
    "\t# so that change can be find easily \n",
    "\tgray = cv2.GaussianBlur(gray, (21, 21), 0) \n",
    "\n",
    "\t# In first iteration we assign the value \n",
    "\t# of static_back to our first frame \n",
    "\tif static_back is None: \n",
    "\t\tstatic_back = gray \n",
    "\t\tcontinue\n",
    "\n",
    "\t# Difference between static background \n",
    "\t# and current frame(which is GaussianBlur) \n",
    "\tdiff_frame = cv2.absdiff(static_back, gray) \n",
    "\n",
    "\t# If change in between static background and \n",
    "\t# current frame is greater than 30 it will show white color(255) \n",
    "\tthresh_frame = cv2.threshold(diff_frame, 30, 255, cv2.THRESH_BINARY)[1] \n",
    "\tthresh_frame = cv2.dilate(thresh_frame, None, iterations = 2) \n",
    "\n",
    "\t# Finding contour of moving object \n",
    "\t(_, cnts, _) = cv2.findContours(thresh_frame.copy(), \n",
    "\t\t\t\t\tcv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) \n",
    "\n",
    "\tfor contour in cnts: \n",
    "\t\tif cv2.contourArea(contour) < 16000: \n",
    "\t\t\tcontinue\n",
    "\t\tmotion = 1\n",
    "\n",
    "\t\t(x, y, w, h) = cv2.boundingRect(contour) \n",
    "\t\t# making green rectangle arround the moving object \n",
    "\t\tcv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3) \n",
    "\n",
    "\t# Appending status of motion \n",
    "\tmotion_list.append(motion) \n",
    "\n",
    "\tmotion_list = motion_list[-2:] \n",
    "\n",
    "\t# Appending Start time of motion \n",
    "\tif motion_list[-1] == 1 and motion_list[-2] == 0: \n",
    "\t\ttime.append(datetime.now()) \n",
    "\n",
    "\t# Appending End time of motion \n",
    "\tif motion_list[-1] == 0 and motion_list[-2] == 1: \n",
    "\t\ttime.append(datetime.now()) \n",
    "\n",
    "\t# Displaying image in gray_scale \n",
    "\tcv2.imshow(\"Gray Frame\", gray) \n",
    "\n",
    "\t# Displaying the difference in currentframe to \n",
    "\t# the staticframe(very first_frame) \n",
    "\tcv2.imshow(\"Difference Frame\", diff_frame) \n",
    "\n",
    "\t# Displaying the black and white image in which if \n",
    "\t# intencity difference greater than 30 it will appear white \n",
    "\tcv2.imshow(\"Threshold Frame\", thresh_frame) \n",
    "\n",
    "\t# Displaying color frame with contour of motion of object \n",
    "\tcv2.imshow(\"Color Frame\", frame) \n",
    "\n",
    "\tkey = cv2.waitKey(1) \n",
    "\t# if q entered whole process will stop \n",
    "\tif key == ord('q'): \n",
    "\t\t# if something is movingthen it append the end time of movement \n",
    "\t\tif motion == 1: \n",
    "\t\t\ttime.append(datetime.now()) \n",
    "\t\tbreak\n",
    "\n",
    "# Appending time of motion in DataFrame \n",
    "for i in range(0, len(time), 2): \n",
    "\tdf = df.append({\"Start\":time[i], \"End\":time[i + 1]}, ignore_index = True) \n",
    "\n",
    "# Creating a csv file in which time of movements will be saved \n",
    "df.to_csv(\"Time_of_movements.csv\") \n",
    "\n",
    "video.release() \n",
    "\n",
    "# Destroying all the windows \n",
    "cv2.destroyAllWindows() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
